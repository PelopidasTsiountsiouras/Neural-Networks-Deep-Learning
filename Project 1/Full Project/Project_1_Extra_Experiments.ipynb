{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORKS PROJECT\n",
        "Load Existing Models + Strategic New Experiments\n",
        "\n",
        "Features:\n",
        "- Load pre-trained models (safe backup)\n",
        "- 2-3 new strategic experiments (hidden layers, learning rate)\n",
        "\n",
        "SETUP:\n",
        "1. Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "2. Run all cells\n",
        "3. Existing models will be loaded, new ones trained"
      ],
      "metadata": {
        "id": "Pspa5lDmb0nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LIBRARIES**"
      ],
      "metadata": {
        "id": "5UzVHPhCcBpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "mKxhZNxXcJJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GOOGLE DRIVE SETUP & BACKUP STRATEGY**"
      ],
      "metadata": {
        "id": "F-oXONIrcUyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K23u1gqjblrt",
        "outputId": "87a21f23-6a0e-4a9e-ba93-4d6bc7f4e30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Project root: /content/drive/MyDrive/NN_CIFAR10_Project\n",
            "Existing models: /content/drive/MyDrive/NN_CIFAR10_Project/trained_models\n",
            "New experiments: /content/drive/MyDrive/NN_CIFAR10_Project/new_experiments\n",
            "Session timestamp: 20251123_065646\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Project folders\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/NN_CIFAR10_Project'\n",
        "MODELS_DIR = f'{PROJECT_ROOT}/trained_models'  # Safe backup\n",
        "RESULTS_DIR = f'{PROJECT_ROOT}/results'\n",
        "NEW_EXPERIMENTS_DIR = f'{PROJECT_ROOT}/new_experiments'\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [PROJECT_ROOT, MODELS_DIR, RESULTS_DIR, NEW_EXPERIMENTS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Existing models: {MODELS_DIR}\")\n",
        "print(f\"New experiments: {NEW_EXPERIMENTS_DIR}\")\n",
        "\n",
        "# Backup timestamp\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "print(f\"Session timestamp: {TIMESTAMP}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPU SETUP**"
      ],
      "metadata": {
        "id": "b6yQlApMcggO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nğŸ–¥ï¸  Device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    print(\"GPU not available! Switch to GPU in Runtime settings.\")\n",
        "\n",
        "# Seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbiQwMNMckWK",
        "outputId": "9c2bda04-3d0b-4e48-8794-ab1f238d7f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ–¥ï¸  Device: cpu\n",
            "GPU not available! Switch to GPU in Runtime settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL ARCHITECTURES**"
      ],
      "metadata": {
        "id": "YlBGQxvHcuQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BestMLP(nn.Module):\n",
        "    \"\"\"MLP architecture (must match existing model)\"\"\"\n",
        "    def __init__(self):\n",
        "        super(BestMLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3072, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class BestCNN(nn.Module):\n",
        "    \"\"\"CNN architecture (must match existing model)\"\"\"\n",
        "    def __init__(self):\n",
        "        super(BestCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.dropout2d(x, 0.1, training=self.training)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.dropout2d(x, 0.2, training=self.training)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.dropout2d(x, 0.3, training=self.training)\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "# NEW: Configurable MLP for experiments\n",
        "class ConfigurableMLP(nn.Module):\n",
        "    \"\"\"MLP with configurable hidden layers\"\"\"\n",
        "    def __init__(self, hidden_sizes=[512], dropout=0.3):\n",
        "        super(ConfigurableMLP, self).__init__()\n",
        "        layers = [nn.Flatten()]\n",
        "\n",
        "        in_size = 3072\n",
        "        for h_size in hidden_sizes:\n",
        "            layers.extend([\n",
        "                nn.Linear(in_size, h_size),\n",
        "                nn.BatchNorm1d(h_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            in_size = h_size\n",
        "\n",
        "        layers.append(nn.Linear(in_size, 10))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "SxZUhJHjc69A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOAD EXISTING MODELS & RESULTS**"
      ],
      "metadata": {
        "id": "VBaWuFqSdBAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "existing_results = {}\n",
        "loaded_models = {}\n",
        "\n",
        "# Try to load existing models\n",
        "existing_model_files = {\n",
        "    'MLP-Best': 'MLP-Best_model.pth',\n",
        "    'CNN-Best-100ep': 'CNN-Best-100ep_model.pth',\n",
        "    'CNN-Best-150ep': 'CNN-Best-150ep_model.pth'\n",
        "}\n",
        "\n",
        "for name, filename in existing_model_files.items():\n",
        "    model_path = f'{PROJECT_ROOT}/{filename}'\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            if 'MLP' in name:\n",
        "                model = BestMLP().to(device)\n",
        "            else:\n",
        "                model = BestCNN().to(device)\n",
        "\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            model.eval()\n",
        "\n",
        "            loaded_models[name] = model\n",
        "            print(f\"Loaded: {name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {name}: {e}\")\n",
        "    else:\n",
        "        print(f\"Not found: {name} (will need to train)\")\n",
        "\n",
        "# Try to load existing results\n",
        "results_file = f'{PROJECT_ROOT}/complete_results.pkl'\n",
        "if os.path.exists(results_file):\n",
        "    try:\n",
        "        with open(results_file, 'rb') as f:\n",
        "            saved_data = pickle.load(f)\n",
        "            existing_results = saved_data.get('nn_results', {})\n",
        "        print(f\"\\nLoaded existing results: {len(existing_results)} experiments\")\n",
        "    except:\n",
        "        print(\"\\nCould not load existing results\")\n",
        "\n",
        "# Load baseline results\n",
        "baseline_results = {}\n",
        "baseline_file = f'{PROJECT_ROOT}/baseline_results.pkl'\n",
        "if os.path.exists(baseline_file):\n",
        "    with open(baseline_file, 'rb') as f:\n",
        "        baseline_results = pickle.load(f)\n",
        "    print(f\"Loaded baseline results\")\n",
        "else:\n",
        "    print(\"Baseline results not found - will compute\")\n",
        "\n",
        "print(f\"\\nStatus:\")\n",
        "print(f\"Loaded models: {len(loaded_models)}\")\n",
        "print(f\"Existing results: {len(existing_results)}\")\n",
        "print(f\"Baseline results: {len(baseline_results)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HP-wWvAdFT3",
        "outputId": "99f74bbc-03cc-4500-c2c6-f6ab73231d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: MLP-Best\n",
            "Loaded: CNN-Best-100ep\n",
            "Loaded: CNN-Best-150ep\n",
            "\n",
            "Could not load existing results\n",
            "Baseline results not found - will compute\n",
            "\n",
            "Status:\n",
            "Loaded models: 3\n",
            "Existing results: 0\n",
            "Baseline results: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA LOADING**"
      ],
      "metadata": {
        "id": "AnoIqMUadQqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_size = int(0.9 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "trainset, valset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train: {train_size:,} | Val: {val_size:,} | Test: {len(testset):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwhbwrHldUhn",
        "outputId": "1432b7b0-6c8f-4fd8-c5ab-fcc009004fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 45,000 | Val: 5,000 | Test: 10,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING UTILITIES**"
      ],
      "metadata": {
        "id": "e8gIN_vJdZDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuickTrainer:\n",
        "    \"\"\"Fast trainer for experiments\"\"\"\n",
        "    def __init__(self, model, trainloader, valloader, criterion, optimizer, device, scheduler=None):\n",
        "        self.model = model.to(device)\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "        self.best_val_acc = 0.0\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for inputs, labels in self.trainloader:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "        return running_loss / total, 100. * correct / total\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.valloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "        return running_loss / total, 100. * correct / total\n",
        "\n",
        "    def train(self, epochs):\n",
        "        print(f\"Training for {epochs} epochs...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "            val_loss, val_acc = self.validate()\n",
        "\n",
        "            if self.scheduler:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "\n",
        "            if val_acc > self.best_val_acc:\n",
        "                self.best_val_acc = val_acc\n",
        "                self.best_model_state = self.model.state_dict().copy()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                print(f\"  Epoch {epoch+1:3d}/{epochs} | TL: {train_loss:.4f} TA: {train_acc:5.2f}% | VL: {val_loss:.4f} VA: {val_acc:5.2f}%\")\n",
        "\n",
        "        self.model.load_state_dict(self.best_model_state)\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Completed in {elapsed:.1f}s | Best Val: {self.best_val_acc:.2f}%\")\n",
        "        return self.history, elapsed\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"Evaluate model and return detailed results\"\"\"\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = 100. * np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': np.array(all_preds),\n",
        "        'labels': np.array(all_labels),\n",
        "        'probabilities': np.array(all_probs),\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ],
      "metadata": {
        "id": "Wzmyqir4dYxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMPUTE BASELINES**"
      ],
      "metadata": {
        "id": "olRfTOawdkvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not baseline_results:\n",
        "    # Prepare data for baselines\n",
        "    transform_basic = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    trainset_basic = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_basic)\n",
        "    testset_basic = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_basic)\n",
        "\n",
        "    def prepare_data(dataset, max_samples=2000):\n",
        "        loader = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
        "        X_list, y_list = [], []\n",
        "        for images, labels in loader:\n",
        "            X_list.append(images.numpy())\n",
        "            y_list.append(labels.numpy())\n",
        "            if max_samples and len(y_list) * 1000 >= max_samples:\n",
        "                break\n",
        "        X = np.vstack(X_list).reshape(len(np.hstack(y_list)), -1)\n",
        "        y = np.hstack(y_list)\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = prepare_data(trainset_basic, max_samples=None)\n",
        "    X_test, y_test = prepare_data(testset_basic, max_samples=2000)\n",
        "\n",
        "    # PCA\n",
        "    print(\"\\nApplying PCA-25...\")\n",
        "    pca = PCA(n_components=25, random_state=42)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    explained_var = pca.explained_variance_ratio_.sum() * 100\n",
        "    print(f\"Explained variance: {explained_var:.2f}%\")\n",
        "\n",
        "\n",
        "    # 1-NN\n",
        "    print(\"\\nRunning 1-NN...\")\n",
        "    start = time.time()\n",
        "    predictions_1nn = []\n",
        "    for i, x in enumerate(X_test_pca):\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"   {i+1}/{len(X_test_pca)}\")\n",
        "        distances = np.sqrt(np.sum((X_train_pca - x) ** 2, axis=1))\n",
        "        k_idx = np.argsort(distances)[:3]\n",
        "        predictions_1nn.append(Counter(y_train[k_idx]).most_common(1)[0][0])\n",
        "    acc_1nn = 100. * np.mean(np.array(predictions_1nn) == y_test)\n",
        "    time_1nn = time.time() - start\n",
        "    print(f\"1-NN: {acc_1nn:.2f}% ({time_1nn:.1f}s)\")\n",
        "\n",
        "    # 3-NN\n",
        "    print(\"\\nRunning 3-NN...\")\n",
        "    start = time.time()\n",
        "    predictions_3nn = []\n",
        "    for i, x in enumerate(X_test_pca):\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"   {i+1}/{len(X_test_pca)}\")\n",
        "        distances = np.sqrt(np.sum((X_train_pca - x) ** 2, axis=1))\n",
        "        k_idx = np.argsort(distances)[:3]\n",
        "        predictions_3nn.append(Counter(y_train[k_idx]).most_common(1)[0][0])\n",
        "    acc_3nn = 100. * np.mean(np.array(predictions_3nn) == y_test)\n",
        "    time_3nn = time.time() - start\n",
        "    print(f\"3-NN: {acc_3nn:.2f}% ({time_3nn:.1f}s)\")\n",
        "\n",
        "    # NCC\n",
        "    print(\"\\nRunning NCC...\")\n",
        "    start = time.time()\n",
        "    centroids = np.array([X_train_pca[y_train == c].mean(axis=0) for c in range(10)])\n",
        "    predictions_ncc = [np.argmin(np.sqrt(np.sum((centroids - x) ** 2, axis=1))) for x in X_test_pca]\n",
        "    acc_ncc = 100. * np.mean(np.array(predictions_ncc) == y_test)\n",
        "    time_ncc = time.time() - start\n",
        "    print(f\"NCC: {acc_ncc:.2f}% ({time_ncc:.1f}s)\")\n",
        "\n",
        "    baseline_results = {\n",
        "        '1-NN (PCA-25)': {'accuracy': acc_1nn, 'time': time_1nn, 'predictions': predictions_1nn},\n",
        "        '3-NN (PCA-25)': {'accuracy': acc_3nn, 'time': time_3nn, 'predictions': predictions_3nn},\n",
        "        'NCC (PCA-25)': {'accuracy': acc_ncc, 'time': time_ncc, 'predictions': predictions_ncc}\n",
        "    }\n",
        "\n",
        "    # Save\n",
        "    with open(f'{RESULTS_DIR}/baseline_results.pkl', 'wb') as f:\n",
        "        pickle.dump(baseline_results, f)\n",
        "    print(f\"\\nBaselines saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEg7b-uUdpGR",
        "outputId": "5bfee823-8af1-43a8-ce31-cb6afc0e6cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying PCA-25...\n",
            "Explained variance: 77.11%\n",
            "\n",
            "Running 1-NN...\n",
            "   500/2000\n",
            "   1000/2000\n",
            "   1500/2000\n",
            "   2000/2000\n",
            "1-NN: 40.05% (16.1s)\n",
            "\n",
            "Running 3-NN...\n",
            "   500/2000\n",
            "   1000/2000\n",
            "   1500/2000\n",
            "   2000/2000\n",
            "3-NN: 40.05% (14.7s)\n",
            "\n",
            "Running NCC...\n",
            "NCC: 27.90% (0.1s)\n",
            "\n",
            "Baselines saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEW EXPERIMENTS**"
      ],
      "metadata": {
        "id": "5enoWQF2edKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_experiments = {\n",
        "    # Experiment 1: MLP with different hidden layers (~30 min)\n",
        "    'MLP-Small-256': {\n",
        "        'model_class': ConfigurableMLP,\n",
        "        'model_args': {'hidden_sizes': [256], 'dropout': 0.3},\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'lr': 0.001,\n",
        "        'optimizer': 'AdamW'\n",
        "    },\n",
        "\n",
        "    # Experiment 2: MLP with larger architecture (~35 min)\n",
        "    'MLP-Large-1024-512': {\n",
        "        'model_class': ConfigurableMLP,\n",
        "        'model_args': {'hidden_sizes': [1024, 512], 'dropout': 0.3},\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'lr': 0.001,\n",
        "        'optimizer': 'AdamW'\n",
        "    },\n",
        "\n",
        "    # Experiment 3: MLP with different learning rate (~30 min)\n",
        "    'MLP-512-LR001': {\n",
        "        'model_class': ConfigurableMLP,\n",
        "        'model_args': {'hidden_sizes': [512], 'dropout': 0.3},\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'lr': 0.01,  # Higher LR\n",
        "        'optimizer': 'AdamW'\n",
        "    },\n",
        "}\n",
        "\n",
        "print(f\"\\nPlanned experiments: {len(new_experiments)}\")\n",
        "print(f\"Estimated time: ~90-120 minutes total\")\n",
        "print(\"\\nExperiments:\")\n",
        "for i, name in enumerate(new_experiments.keys(), 1):\n",
        "    print(f\"   {i}. {name}\")\n",
        "\n",
        "new_results = {}\n",
        "\n",
        "for exp_name, config in new_experiments.items():\n",
        "    print(f\"\\n{'â”'*70}\")\n",
        "    print(f\"EXPERIMENT: {exp_name}\")\n",
        "    print(f\"{'â”'*70}\")\n",
        "\n",
        "    # Check if already exists\n",
        "    model_file = f'{NEW_EXPERIMENTS_DIR}/{exp_name}_model.pth'\n",
        "    if os.path.exists(model_file):\n",
        "        print(f\"Already trained! Loading from cache...\")\n",
        "        model = config['model_class'](**config['model_args']).to(device)\n",
        "        model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "        # Load results\n",
        "        results_file = f'{NEW_EXPERIMENTS_DIR}/{exp_name}_results.pkl'\n",
        "        if os.path.exists(results_file):\n",
        "            with open(results_file, 'rb') as f:\n",
        "                new_results[exp_name] = pickle.load(f)\n",
        "            print(f\"Loaded cached results\")\n",
        "            continue\n",
        "\n",
        "    # Create model\n",
        "    model = config['model_class'](**config['model_args']).to(device)\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Parameters: {params:,}\")\n",
        "\n",
        "    # Data loaders\n",
        "    trainloader = DataLoader(trainset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
        "    valloader = DataLoader(valset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "    testloader = DataLoader(testset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    if config['optimizer'] == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=5e-4)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
        "\n",
        "    # Train\n",
        "    trainer = QuickTrainer(model, trainloader, valloader, criterion, optimizer, device, scheduler)\n",
        "    history, train_time = trainer.train(epochs=config['epochs'])\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"\\nEvaluating on test set...\")\n",
        "    test_results = evaluate_model(model, testloader, device)\n",
        "\n",
        "    # Store results\n",
        "    new_results[exp_name] = {\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'test_accuracy': test_results['accuracy'],\n",
        "        'best_val_accuracy': trainer.best_val_acc,\n",
        "        'training_time': train_time,\n",
        "        'test_results': test_results,\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "    print(f\"Test Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "\n",
        "    # Save model & results\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    with open(f'{NEW_EXPERIMENTS_DIR}/{exp_name}_results.pkl', 'wb') as f:\n",
        "        pickle.dump(new_results[exp_name], f)\n",
        "    print(f\"Saved: {exp_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZe6fHPVegmK",
        "outputId": "dd615adb-2cb2-44f5-dcb3-5bf1cbcbb6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Planned experiments: 3\n",
            "Estimated time: ~90-120 minutes total\n",
            "\n",
            "Experiments:\n",
            "   1. MLP-Small-256\n",
            "   2. MLP-Large-1024-512\n",
            "   3. MLP-512-LR001\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "EXPERIMENT: MLP-Small-256\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "Parameters: 789,770\n",
            "Training for 50 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch   1/50 | TL: 1.9966 TA: 31.07% | VL: 1.8872 VA: 36.06%\n",
            "  Epoch  10/50 | TL: 1.7934 TA: 41.46% | VL: 1.7219 VA: 45.12%\n",
            "  Epoch  20/50 | TL: 1.7502 TA: 44.09% | VL: 1.6618 VA: 48.74%\n",
            "  Epoch  30/50 | TL: 1.7169 TA: 45.59% | VL: 1.6504 VA: 49.02%\n",
            "  Epoch  40/50 | TL: 1.6992 TA: 46.47% | VL: 1.6275 VA: 50.06%\n",
            "  Epoch  50/50 | TL: 1.6966 TA: 46.76% | VL: 1.6174 VA: 50.46%\n",
            "Completed in 2400.0s | Best Val: 50.76%\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 52.52%\n",
            "ğŸ’¾ Saved: MLP-Small-256\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "EXPERIMENT: MLP-Large-1024-512\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "Parameters: 3,679,754\n",
            "Training for 50 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch   1/50 | TL: 1.9774 TA: 31.53% | VL: 1.8248 VA: 38.82%\n",
            "  Epoch  10/50 | TL: 1.6917 TA: 44.98% | VL: 1.6325 VA: 47.48%\n",
            "  Epoch  20/50 | TL: 1.6214 TA: 48.82% | VL: 1.5616 VA: 51.56%\n",
            "  Epoch  30/50 | TL: 1.5695 TA: 51.54% | VL: 1.5072 VA: 54.46%\n",
            "  Epoch  40/50 | TL: 1.5314 TA: 53.32% | VL: 1.4756 VA: 56.34%\n",
            "  Epoch  50/50 | TL: 1.5184 TA: 53.99% | VL: 1.4626 VA: 56.24%\n",
            "Completed in 3257.3s | Best Val: 56.44%\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 58.05%\n",
            "ğŸ’¾ Saved: MLP-Large-1024-512\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "EXPERIMENT: MLP-512-LR001\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "Parameters: 1,579,530\n",
            "Training for 50 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch   1/50 | TL: 2.0454 TA: 29.60% | VL: 1.8744 VA: 36.86%\n",
            "  Epoch  10/50 | TL: 1.8050 TA: 41.01% | VL: 1.7325 VA: 44.06%\n",
            "  Epoch  20/50 | TL: 1.7585 TA: 43.20% | VL: 1.6804 VA: 46.40%\n",
            "  Epoch  30/50 | TL: 1.7165 TA: 45.36% | VL: 1.6401 VA: 48.66%\n",
            "  Epoch  40/50 | TL: 1.6830 TA: 47.11% | VL: 1.5948 VA: 50.74%\n",
            "  Epoch  50/50 | TL: 1.6628 TA: 48.28% | VL: 1.5911 VA: 51.56%\n",
            "Completed in 2692.0s | Best Val: 53.14%\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 54.44%\n",
            "ğŸ’¾ Saved: MLP-512-LR001\n"
          ]
        }
      ]
    }
  ]
}